{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15688 Tutorial Introduction to Pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This tutorial will introduce you the basic concepts of pyspark as well as useful examples using python code. I am sure that you have heard the concept of big data. The complexity and volume of new data generate the requirement of new technology. The ability of dealing with big data is essential to each company and industry. Pyspark is one of the most common technology in this area so in this tutorial will give you a pyspark 101. The package used here is pyspark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Agenda\n",
    "\n",
    "- Apache Spark Architecture\n",
    "- The Key Data Object - RDD\n",
    "- Operations on RDD\n",
    "- The Key Data Object - DataFrame\n",
    "- Operations on DataFrame\n",
    "- PySpark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark Architecture\n",
    "\n",
    "Spark achieves high-speed data processing thourgh distributed computation. The high-level architecture is driver-worker, a node controlling over other processes(workers). The driver organizes what computational work should been done on which worker. The computational job is distributed(map) to workers and the partial results are combined(reduce) to get final output. In this tutorial we will use pyspark, a Spark library written in Python providing easy-to-write spark programming.\n",
    "\n",
    "Frist of all, spark creates SparkContext object, it tells the computer where to find a cluster( a driver and its worker brothers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f43c326609f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/opt/packages/spark/latest/python/pyspark\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\coding\\python\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    131\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mC:\\Users\\coding\\python\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\coding\\python\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[1;31m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m                 \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/opt/packages/spark/latest/python/lib/py4j-0.10.9-src.zip\")\n",
    "sys.path.append(\"/opt/packages/spark/latest/python/\")\n",
    "sys.path.append(\"/opt/packages/spark/latest/python/pyspark\")\n",
    "from pyspark import SparkConf, SparkContext\n",
    "sc = SparkContext()\n",
    "sc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Key Data Object - RDD\n",
    "\n",
    "SparkContext can be used to create RDD(Resilient Distributed Dataset). RDD is a fundamental data structure of Spark. It is immutable, distributed and fault-tolerant. So once you create a RDD, you are not able to change it. RDD is divided into several partitions across a cluster. The number of partitions should be decided by programmer and is not changeable as well. If you forget to do this, the machine will decide it by itself according to abailable capacity. The more partitions, the better ability of parallelism. The thrid property of RDD is fault-tolerant. If one worker crushes, it simplily restart its process without effecting other workers. From other aspects, you can just imagine RDD as a list in python.\n",
    "\n",
    "You can create a RDD by two ways. The first one is to parallelizing Python collections, such as list.( See! RDD and list are similar.) Another way is to import from files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b844e8548619>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'I'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'like'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'practical'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'science'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrdd1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordsList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# the number of partitions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "# parallelizing\n",
    "data1 = ['I', 'like', 'practical', 'data', 'science','like','lot','a','good','good']\n",
    "rdd1 = sc.parallelize(data1, 4) # the number of partitions\n",
    "\n",
    "data2 = [1,2,3,4]\n",
    "rdd2 = sc.parallelize(data2) # do not decide number of partitions\n",
    "print(\"number of partitions:\"+str(rdd2.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opeartions on RDD\n",
    "\n",
    "There are two opeartions on RDD. Transformations and actions. An interesting thing is that transformations are lazy. They are not computed immediately until an action runs on it. Another useful method is cache. By caching data that may be reused you keep them in memory and reduce time of disk I/O.\n",
    "\n",
    "The basic tranformations include map(), filter(), distinct(), flatMap(). The input of these transformations is a function. Map returns a new rdd by passing each element into the function. Filter returns a new rdd with elements that return True through filter. Distinct returns distinct elements of original dataset. FlatMap is similar to map but the mapping is not restricted to one-to-one. Let's see some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map\n",
    "mapRDD = rdd1.map(lambda x: x+'s')\n",
    "print (mapRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter\n",
    "print(rdd2.filter(lambda x: x <= 2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distinct\n",
    "print(rdd1.distinct().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the differences between map and flatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FlatMap\n",
    "\n",
    "print(rdd2.map(lambda x: [x,x+1]).collect())\n",
    "print(rdd2.flatmap(lambda x: [x,x+1]).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be confused by the action \"collect()\". Remember? We mentioned before that transformations are lazy. They are not executed until an action runs on them. Here collect is an action, it just gives all the elements in a RDD. Other common actions are reduce(function), take(n), and takeOrdered(n,key = function). Reduce function takes two parameters and gives one, it is widely used to implement basic addment. Take returns the first n elements. Takeorderd is similar and it allows you to create your own sort key(ascending). You can change it into descending with a small trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce\n",
    "print(rdd2.reduce(lambda x,y:x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take\n",
    "print(rdd1.take(2))\n",
    "\n",
    "#takeOrderd\n",
    "print(rdd1.takeOrderd(2,lambda x: -1*x)) # a trick to descend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact pyspark provides some methods that allow key-value pair transforamtions. Like dictionary in python. They are reduceByKey(),sortByKey and groupByKey(). ReduceByKey is just like reduce, only this time values are combined by their keys. SortByKey() and groupByKey() works the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create rdd with keys\n",
    "data3 = [(1,2),(3,4),(5,6),(5,7),(3,8)]\n",
    "keyRDD = sc.parallelize(data3)\n",
    "#reducebykey\n",
    "print(keyRDD.reduceByKey(lambda x,y : x+y).collect())\n",
    "#sortbykey\n",
    "print(keyRDD.sortByKey().collect())\n",
    "#groupbykey\n",
    "print(keyRDD.groupByKey().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a command of all useful functions. Let's combine them together and do something interesting. A famous example of distributed computation is word frequency count. In the comments below I give you the code to create RDD from local files. Since I cannot submit local files in this tutorial, I will create the RDD by hand. The text is copied from our course website.\n",
    "\n",
    "What are the steps to do word count?\n",
    "\n",
    "First I will separate the original sentence into "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = test.txt'\n",
    "#rdd = sc.textFile(file)\n",
    "\n",
    "data4 = ['With data science, as with most disciplines, the best way to learn a particular topic is to teach it to others. The tutorial assignment of this course is meant to give you the chance to create a short tutorial on something related to data science. You can describe how to use a particular algorithm, library, methodology, or data set. Your tutorial should provide an introduction to this topic suitable for the level of other students taking this course. Ultimately, you will be graded by the course instructor and other students taking the course']\n",
    "rdd4 = sc.parallelize(data4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
